{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torch.functional as F\n",
    "import torch.nn.functional as F\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import reuters\n",
    "corpus = [\n",
    "    'he is a king',\n",
    "    'she is a queen',\n",
    "    'he is a man',\n",
    "    'she is a woman',\n",
    "    'warsaw is poland capital',\n",
    "    'berlin is germany capital',\n",
    "    'paris is france capital',   \n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['he', 'is', 'a', 'king'], ['she', 'is', 'a', 'queen'], ['he', 'is', 'a', 'man'], ['she', 'is', 'a', 'woman'], ['warsaw', 'is', 'poland', 'capital'], ['berlin', 'is', 'germany', 'capital'], ['paris', 'is', 'france', 'capital']]\n"
     ]
    }
   ],
   "source": [
    "def tokenize_corpus(corpus):\n",
    "    tokens = [x.split() for x in corpus]\n",
    "    return tokens\n",
    "\n",
    "tokenized_corpus = tokenize_corpus(corpus)\n",
    "print(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = []\n",
    "for sentence in tokenized_corpus:\n",
    "    for token in sentence:\n",
    "        if token not in vocabulary:\n",
    "            vocabulary.append(token)\n",
    "\n",
    "word2idx = {w: idx for (idx, w) in enumerate(vocabulary)}\n",
    "idx2word = {idx: w for (idx, w) in enumerate(vocabulary)}\n",
    "\n",
    "vocabulary_size = len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "window_size = 2\n",
    "idx_pairs = []\n",
    "# for each sentence\n",
    "for sentence in tokenized_corpus:\n",
    "    indices = [word2idx[word] for word in sentence]\n",
    "    # for each word, threated as center word\n",
    "    for center_word_pos in range(len(indices)):\n",
    "        # for each window position\n",
    "        for w in range(-window_size, window_size + 1):\n",
    "            context_word_pos = center_word_pos + w\n",
    "            # make soure not jump out sentence\n",
    "            if context_word_pos < 0 or context_word_pos >= len(indices) or center_word_pos == context_word_pos:\n",
    "                continue\n",
    "            context_word_idx = indices[context_word_pos]\n",
    "            idx_pairs.append((indices[center_word_pos], context_word_idx))\n",
    "\n",
    "idx_pairs = np.array(idx_pairs) # it will be useful to have this as numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_layer(word_idx):\n",
    "    x = torch.zeros(vocabulary_size).float()\n",
    "    x[word_idx] = 1.0\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epo =0 loss = 4.414617017337254\n",
      "epo =10 loss = 2.812441304751805\n",
      "epo =20 loss = 2.4322106616837638\n",
      "epo =30 loss = 2.23747455903462\n",
      "epo =40 loss = 2.1135182227407183\n",
      "epo =50 loss = 2.029567575454712\n",
      "epo =60 loss = 1.968772886480604\n",
      "epo =70 loss = 1.9210169128009251\n",
      "epo =80 loss = 1.8814312934875488\n",
      "epo =90 loss = 1.8480552094323295\n",
      "epo =100 loss = 1.8200960516929627\n",
      "epo =110 loss = 1.7969684038843428\n",
      "epo =120 loss = 1.7779400382723127\n",
      "epo =130 loss = 1.7621682252202715\n",
      "epo =140 loss = 1.7488552689552308\n",
      "epo =150 loss = 1.737350594997406\n",
      "epo =160 loss = 1.727170237473079\n",
      "epo =170 loss = 1.7179731096540178\n",
      "epo =180 loss = 1.709526412827628\n",
      "epo =190 loss = 1.701675043787275\n",
      "epo =200 loss = 1.6943196586200169\n",
      "epo =210 loss = 1.687399433340345\n",
      "epo =220 loss = 1.6808804614203317\n",
      "epo =230 loss = 1.6747456686837332\n",
      "epo =240 loss = 1.6689882891518728\n",
      "epo =250 loss = 1.663605192729405\n",
      "epo =260 loss = 1.6585930057934353\n",
      "epo =270 loss = 1.6539454392024449\n",
      "epo =280 loss = 1.6496514575822012\n",
      "epo =290 loss = 1.6456959128379822\n",
      "epo =300 loss = 1.6420601333890643\n",
      "epo =310 loss = 1.63872332743236\n",
      "epo =320 loss = 1.635664394923619\n",
      "epo =330 loss = 1.6328623226710728\n",
      "epo =340 loss = 1.630297134603773\n",
      "epo =350 loss = 1.6279499224254064\n",
      "epo =360 loss = 1.6258030925478255\n",
      "epo =370 loss = 1.6238400152751378\n",
      "epo =380 loss = 1.6220452291624887\n",
      "epo =390 loss = 1.6204038977622985\n",
      "epo =400 loss = 1.6189025010381426\n",
      "epo =410 loss = 1.61752815757479\n",
      "epo =420 loss = 1.6162689208984375\n",
      "epo =430 loss = 1.615113948072706\n",
      "epo =440 loss = 1.6140530603272574\n",
      "epo =450 loss = 1.6130769763674055\n",
      "epo =460 loss = 1.612177506514958\n",
      "epo =470 loss = 1.6113470571381705\n",
      "epo =480 loss = 1.6105787175042288\n",
      "epo =490 loss = 1.609866578238351\n",
      "epo =500 loss = 1.6092049138886588\n",
      "epo =510 loss = 1.6085889305387224\n",
      "epo =520 loss = 1.6080142515046256\n",
      "epo =530 loss = 1.6074769105230058\n",
      "epo =540 loss = 1.6069734488214766\n",
      "epo =550 loss = 1.6065007635525295\n",
      "epo =560 loss = 1.6060558983257838\n",
      "epo =570 loss = 1.6056364740644182\n",
      "epo =580 loss = 1.6052401065826416\n",
      "epo =590 loss = 1.6048649430274964\n",
      "epo =600 loss = 1.604509026663644\n",
      "epo =610 loss = 1.6041708265032086\n",
      "epo =620 loss = 1.603848831994193\n",
      "epo =630 loss = 1.603541648387909\n",
      "epo =640 loss = 1.6032481363841466\n",
      "epo =650 loss = 1.602967187336513\n",
      "epo =660 loss = 1.6026978084019252\n",
      "epo =670 loss = 1.6024391634123667\n",
      "epo =680 loss = 1.6021903787340437\n",
      "epo =690 loss = 1.6019507663590566\n",
      "epo =700 loss = 1.6017196025167193\n",
      "epo =710 loss = 1.601496251991817\n",
      "epo =720 loss = 1.601280188560486\n",
      "epo =730 loss = 1.6010709524154663\n",
      "epo =740 loss = 1.600867944104331\n",
      "epo =750 loss = 1.6006708179201399\n",
      "epo =760 loss = 1.6004791975021362\n",
      "epo =770 loss = 1.6002925753593444\n",
      "epo =780 loss = 1.600110718182155\n",
      "epo =790 loss = 1.5999334522656032\n",
      "epo =800 loss = 1.599760261603764\n",
      "epo =810 loss = 1.5995909605707441\n",
      "epo =820 loss = 1.5994253158569336\n",
      "epo =830 loss = 1.5992630345480783\n",
      "epo =840 loss = 1.5991039889199392\n",
      "epo =850 loss = 1.5989480325153895\n",
      "epo =860 loss = 1.598794937133789\n",
      "epo =870 loss = 1.5986444864954268\n",
      "epo =880 loss = 1.598496459211622\n",
      "epo =890 loss = 1.5983508365494865\n",
      "epo =900 loss = 1.5982074788638523\n",
      "epo =910 loss = 1.5980662431035724\n",
      "epo =920 loss = 1.597926960672651\n",
      "epo =930 loss = 1.5977895310946872\n",
      "epo =940 loss = 1.5976538760321481\n",
      "epo =950 loss = 1.5975199239594595\n",
      "epo =960 loss = 1.5973875488553728\n",
      "epo =970 loss = 1.5972566264016288\n",
      "epo =980 loss = 1.5971272110939025\n",
      "epo =990 loss = 1.596999042374747\n"
     ]
    }
   ],
   "source": [
    "embedding_dims = 5\n",
    "W1 = Variable(torch.randn(embedding_dims, vocabulary_size).float(), requires_grad=True)\n",
    "W2 = Variable(torch.randn(vocabulary_size, embedding_dims).float(), requires_grad=True)\n",
    "num_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "for epo in range(num_epochs):\n",
    "    loss_val = 0\n",
    "    for data, target in idx_pairs:\n",
    "        x = Variable(get_input_layer(data)).float()\n",
    "        y_true = Variable(torch.from_numpy(np.array([target])).long())\n",
    "\n",
    "        z1 = torch.matmul(W1, x)\n",
    "        z2 = torch.matmul(W2, z1)\n",
    "    \n",
    "        log_softmax = F.log_softmax(z2, dim=0)\n",
    "\n",
    "        loss = F.nll_loss(log_softmax.view(1,-1), y_true)\n",
    "        loss_val += loss.item()\n",
    "        loss.backward()\n",
    "        W1.data -= learning_rate * W1.grad.data\n",
    "        W2.data -= learning_rate * W2.grad.data\n",
    "\n",
    "        W1.grad.data.zero_()\n",
    "        W2.grad.data.zero_()\n",
    "    if epo % 10 == 0:\n",
    "        print('epo =' + str(epo), 'loss = ' + str(loss_val/len(idx_pairs)))\n",
    "#         print(f'Loss at epo {epo}: {loss_val/len(idx_pairs)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6955, -0.6836,  1.3218,  1.5453,  0.2564,  0.5486,  1.8719,  1.7028,\n",
      "          0.3864,  0.9638,  0.6274,  1.1836, -0.2145,  0.2023,  0.1888],\n",
      "        [ 2.0977,  0.4495, -1.2802,  1.9009,  1.2276, -0.5630,  0.2409,  0.7693,\n",
      "          1.6366,  0.4513,  1.7425,  1.0862,  0.0924,  1.1279,  0.7844],\n",
      "        [ 0.5588,  0.5450,  0.9675, -0.1896,  1.8833,  1.8689, -0.3873,  0.4392,\n",
      "         -1.1240,  0.2618, -1.4854, -0.4512,  0.5924, -0.6976,  0.1051],\n",
      "        [-0.3624,  0.2806, -1.1393, -1.0024,  1.0812,  2.4830, -0.1696,  0.6498,\n",
      "          1.1045,  0.4254,  0.9773, -1.5356,  0.9185,  1.6674,  1.0889],\n",
      "        [ 0.4062, -0.2692,  0.1397, -0.7107,  0.5688,  1.1835,  0.6772, -1.3756,\n",
      "          1.0456, -1.8099,  1.1467,  0.0613,  1.1736,  0.2118, -0.9420]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(W1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in method item of Tensor object at 0x7fa45e295dc8>\n"
     ]
    }
   ],
   "source": [
    "print(W1.item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['he', 'is', 'a', 'king', 'she', 'queen', 'man', 'woman', 'warsaw', 'poland', 'capital', 'berlin', 'germany', 'paris', 'france']\n"
     ]
    }
   ],
   "source": [
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.6650, -0.6515,  1.7643, -0.6612, -0.5184,  0.1936, -0.7601,  1.1543,\n",
      "          0.0982,  1.0659,  1.0329,  0.8713, -1.1771,  1.2245, -0.0804],\n",
      "        [-0.7244,  0.0624,  0.6872, -0.3318, -0.5093,  0.6087, -0.5052,  1.7931,\n",
      "          0.3268, -0.1901,  0.4482, -0.5657, -2.5397, -0.2461, -1.9045],\n",
      "        [-1.6269, -0.6633, -0.5409, -0.9472, -1.5115, -0.0293, -1.1832, -0.0305,\n",
      "          0.3386, -1.2924,  1.9099,  0.0343,  1.6747,  1.2596, -0.2227],\n",
      "        [ 0.2623, -0.3287,  1.0322, -0.5111, -0.2392,  1.1337,  0.1848,  0.3596,\n",
      "          0.4911, -1.0449, -0.6596,  1.0359, -0.4786,  2.0907,  0.2720],\n",
      "        [-0.9111,  0.5388, -0.0388, -0.9062, -1.2115, -1.7624, -0.6426, -2.1454,\n",
      "         -0.9097, -0.1549, -2.4633, -0.5267, -1.9401, -1.0004, -0.3105]])\n"
     ]
    }
   ],
   "source": [
    "print(W1.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = W1.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.2215395   0.40352285  0.03259102 -1.550806   -1.7543684  -2.0483897\n",
      "   0.11105409 -1.0981144  -1.8493615   0.8096374   0.26290727  2.3347015\n",
      "  -1.2286545   0.26170424  0.22906108]\n",
      " [ 1.488256    0.5242639  -0.02792292  0.8890925   0.98666835  0.6626822\n",
      "   2.5294178   1.160715   -2.804427    0.4686744  -2.4683678  -0.64828336\n",
      "   0.16361573 -0.62877995  0.30988243]\n",
      " [ 0.94713867 -0.16445951 -1.6735532  -0.21499224  0.03861188  0.27641746\n",
      "   1.8398671   0.5111441   1.3693995   3.1084561   0.74440926  0.40085322\n",
      "   3.400279   -0.03269182  2.30318   ]\n",
      " [ 1.1791431  -0.29078224  2.9281821   0.6400242   0.90373707  0.39219022\n",
      "   1.3582579   0.64811105  1.2456555   0.80786043  0.02840304  1.1714659\n",
      "  -0.63218325 -1.1650461  -2.0187287 ]\n",
      " [ 0.78564245 -0.24239564  1.433433    1.776831    1.531726    1.4062285\n",
      "   0.4802411   1.1585157   0.18140486 -0.21823777  1.6798475   2.0593903\n",
      "  -0.07797164  3.0830123   1.5188963 ]]\n"
     ]
    }
   ],
   "source": [
    "print(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.2215395   1.488256    0.94713867  1.1791431   0.78564245]\n"
     ]
    }
   ],
   "source": [
    "print(embedding[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = embedding.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['he', 'is', 'a', 'king', 'she', 'queen', 'man', 'woman', 'warsaw', 'poland', 'capital', 'berlin', 'germany', 'paris', 'france']\n"
     ]
    }
   ],
   "source": [
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "print(len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a,b):\n",
    "    return np.dot(a,b) / ( (np.dot(a,a) **.5) * (np.dot(b,b) ** .5) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = {}\n",
    "for i in range(len(vocabulary)):\n",
    "    e[vocabulary[i]] = embedding[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.550806    0.8890925  -0.21499224  0.6400242   1.776831  ]\n"
     ]
    }
   ],
   "source": [
    "print(e['king'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7860721060454229\n"
     ]
    }
   ],
   "source": [
    "print(cosine_similarity(e['poland'] - e['berlin'] + e['warsaw'], e['germany']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding2 = W2.data.numpy()\n",
    "b = {}\n",
    "for i in range(len(vocabulary)):\n",
    "    b[vocabulary[i]] = embedding2[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.487155  ,  0.43970457, -0.3071918 ,  0.3034896 ,  0.4648041 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b['he']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.370908\n"
     ]
    }
   ],
   "source": [
    "print(np.dot(b['poland'], b['poland']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 27366980.0\n",
      "1 24337892.0\n",
      "2 24851178.0\n",
      "3 25348252.0\n",
      "4 23403956.0\n",
      "5 18490446.0\n",
      "6 12494085.0\n",
      "7 7479215.0\n",
      "8 4273249.0\n",
      "9 2490020.0\n",
      "10 1558904.625\n",
      "11 1065366.25\n",
      "12 789222.0\n",
      "13 620933.8125\n",
      "14 508837.9375\n",
      "15 428078.8125\n",
      "16 366265.0\n",
      "17 316963.9375\n",
      "18 276498.34375\n",
      "19 242623.84375\n",
      "20 213931.8125\n",
      "21 189373.375\n",
      "22 168215.5\n",
      "23 149907.75\n",
      "24 133968.296875\n",
      "25 120014.5703125\n",
      "26 107752.7734375\n",
      "27 96944.9453125\n",
      "28 87387.2421875\n",
      "29 78919.40625\n",
      "30 71392.109375\n",
      "31 64685.15234375\n",
      "32 58693.51953125\n",
      "33 53330.37109375\n",
      "34 48523.34765625\n",
      "35 44202.81640625\n",
      "36 40312.4296875\n",
      "37 36806.13671875\n",
      "38 33639.546875\n",
      "39 30777.58203125\n",
      "40 28185.3671875\n",
      "41 25834.21875\n",
      "42 23700.8828125\n",
      "43 21760.79296875\n",
      "44 19995.6484375\n",
      "45 18387.8359375\n",
      "46 16917.37890625\n",
      "47 15577.505859375\n",
      "48 14352.5390625\n",
      "49 13232.05078125\n",
      "50 12207.3232421875\n",
      "51 11268.361328125\n",
      "52 10407.634765625\n",
      "53 9618.0244140625\n",
      "54 8893.4638671875\n",
      "55 8227.6630859375\n",
      "56 7615.3916015625\n",
      "57 7052.28955078125\n",
      "58 6534.0439453125\n",
      "59 6056.74169921875\n",
      "60 5616.8232421875\n",
      "61 5210.9423828125\n",
      "62 4836.8896484375\n",
      "63 4491.5673828125\n",
      "64 4172.26025390625\n",
      "65 3877.21875\n",
      "66 3604.308349609375\n",
      "67 3351.830078125\n",
      "68 3118.178466796875\n",
      "69 2901.986572265625\n",
      "70 2701.614501953125\n",
      "71 2515.93212890625\n",
      "72 2343.8212890625\n",
      "73 2184.073486328125\n",
      "74 2035.876220703125\n",
      "75 1898.49755859375\n",
      "76 1770.7958984375\n",
      "77 1652.1285400390625\n",
      "78 1541.85302734375\n",
      "79 1439.4847412109375\n",
      "80 1344.219970703125\n",
      "81 1255.629150390625\n",
      "82 1173.151611328125\n",
      "83 1096.3822021484375\n",
      "84 1024.9312744140625\n",
      "85 958.3574829101562\n",
      "86 896.3287353515625\n",
      "87 838.55908203125\n",
      "88 784.7364501953125\n",
      "89 734.56982421875\n",
      "90 687.8126220703125\n",
      "91 644.195556640625\n",
      "92 603.4545288085938\n",
      "93 565.4190673828125\n",
      "94 529.9049682617188\n",
      "95 496.71502685546875\n",
      "96 465.70806884765625\n",
      "97 436.7654113769531\n",
      "98 409.63824462890625\n",
      "99 384.27679443359375\n",
      "100 360.54449462890625\n",
      "101 338.35968017578125\n",
      "102 317.59906005859375\n",
      "103 298.15130615234375\n",
      "104 279.9448547363281\n",
      "105 262.8966369628906\n",
      "106 246.93643188476562\n",
      "107 231.9804229736328\n",
      "108 217.96653747558594\n",
      "109 204.835693359375\n",
      "110 192.5228729248047\n",
      "111 180.97332763671875\n",
      "112 170.1500244140625\n",
      "113 159.9970703125\n",
      "114 150.46876525878906\n",
      "115 141.54110717773438\n",
      "116 133.16079711914062\n",
      "117 125.29847717285156\n",
      "118 117.91558837890625\n",
      "119 110.97998046875\n",
      "120 104.46780395507812\n",
      "121 98.3543701171875\n",
      "122 92.61427307128906\n",
      "123 87.2148666381836\n",
      "124 82.14146423339844\n",
      "125 77.37346649169922\n",
      "126 72.89330291748047\n",
      "127 68.68299865722656\n",
      "128 64.72127532958984\n",
      "129 60.999664306640625\n",
      "130 57.49453353881836\n",
      "131 54.197181701660156\n",
      "132 51.097991943359375\n",
      "133 48.178279876708984\n",
      "134 45.43147659301758\n",
      "135 42.847312927246094\n",
      "136 40.41484451293945\n",
      "137 38.124324798583984\n",
      "138 35.96977233886719\n",
      "139 33.937496185302734\n",
      "140 32.02336120605469\n",
      "141 30.219871520996094\n",
      "142 28.52124786376953\n",
      "143 26.922422409057617\n",
      "144 25.417104721069336\n",
      "145 23.994918823242188\n",
      "146 22.65481185913086\n",
      "147 21.391681671142578\n",
      "148 20.201574325561523\n",
      "149 19.07981300354004\n",
      "150 18.02155113220215\n",
      "151 17.023193359375\n",
      "152 16.081497192382812\n",
      "153 15.19375991821289\n",
      "154 14.355829238891602\n",
      "155 13.566057205200195\n",
      "156 12.820314407348633\n",
      "157 12.115814208984375\n",
      "158 11.452258110046387\n",
      "159 10.825780868530273\n",
      "160 10.235122680664062\n",
      "161 9.676630973815918\n",
      "162 9.149299621582031\n",
      "163 8.651082992553711\n",
      "164 8.180646896362305\n",
      "165 7.736388206481934\n",
      "166 7.3176140785217285\n",
      "167 6.921368598937988\n",
      "168 6.5473313331604\n",
      "169 6.193596363067627\n",
      "170 5.859699249267578\n",
      "171 5.544134616851807\n",
      "172 5.245760917663574\n",
      "173 4.96392297744751\n",
      "174 4.69767427444458\n",
      "175 4.445871829986572\n",
      "176 4.208714008331299\n",
      "177 3.9835402965545654\n",
      "178 3.771028995513916\n",
      "179 3.569808006286621\n",
      "180 3.3797082901000977\n",
      "181 3.1995248794555664\n",
      "182 3.0296988487243652\n",
      "183 2.868922472000122\n",
      "184 2.716775894165039\n",
      "185 2.573204517364502\n",
      "186 2.4371414184570312\n",
      "187 2.308230400085449\n",
      "188 2.1862518787384033\n",
      "189 2.0710747241973877\n",
      "190 1.9619221687316895\n",
      "191 1.8588281869888306\n",
      "192 1.7608424425125122\n",
      "193 1.6687633991241455\n",
      "194 1.5813007354736328\n",
      "195 1.4984805583953857\n",
      "196 1.4199527502059937\n",
      "197 1.3457379341125488\n",
      "198 1.2754701375961304\n",
      "199 1.2089335918426514\n",
      "200 1.1458464860916138\n",
      "201 1.0860528945922852\n",
      "202 1.0297116041183472\n",
      "203 0.9762039184570312\n",
      "204 0.9255038499832153\n",
      "205 0.8774926662445068\n",
      "206 0.8319627642631531\n",
      "207 0.7889143228530884\n",
      "208 0.7481893301010132\n",
      "209 0.7094700336456299\n",
      "210 0.6728664636611938\n",
      "211 0.6381718516349792\n",
      "212 0.605273962020874\n",
      "213 0.5741846561431885\n",
      "214 0.5445809960365295\n",
      "215 0.5166356563568115\n",
      "216 0.49004971981048584\n",
      "217 0.4649147391319275\n",
      "218 0.44110140204429626\n",
      "219 0.41849711537361145\n",
      "220 0.3970760703086853\n",
      "221 0.37679630517959595\n",
      "222 0.3575647175312042\n",
      "223 0.33933523297309875\n",
      "224 0.32203009724617004\n",
      "225 0.3056296706199646\n",
      "226 0.2900420129299164\n",
      "227 0.2753264009952545\n",
      "228 0.261269748210907\n",
      "229 0.24801105260849\n",
      "230 0.2354516088962555\n",
      "231 0.22350028157234192\n",
      "232 0.21220353245735168\n",
      "233 0.20145893096923828\n",
      "234 0.19124026596546173\n",
      "235 0.18160173296928406\n",
      "236 0.17246612906455994\n",
      "237 0.16369467973709106\n",
      "238 0.15546199679374695\n",
      "239 0.14762474596500397\n",
      "240 0.14016468822956085\n",
      "241 0.13312652707099915\n",
      "242 0.12643806636333466\n",
      "243 0.12007541954517365\n",
      "244 0.11406771838665009\n",
      "245 0.10833501070737839\n",
      "246 0.10289852321147919\n",
      "247 0.09773986786603928\n",
      "248 0.09285084158182144\n",
      "249 0.08822883665561676\n",
      "250 0.08383438736200333\n",
      "251 0.07963156700134277\n",
      "252 0.0756581500172615\n",
      "253 0.0718705952167511\n",
      "254 0.06828340888023376\n",
      "255 0.06489423662424088\n",
      "256 0.06165023148059845\n",
      "257 0.058609697967767715\n",
      "258 0.05569804459810257\n",
      "259 0.052941519767045975\n",
      "260 0.050322044640779495\n",
      "261 0.047798871994018555\n",
      "262 0.045444559305906296\n",
      "263 0.0431869737803936\n",
      "264 0.0410701222717762\n",
      "265 0.03904356807470322\n",
      "266 0.03712523728609085\n",
      "267 0.03529650345444679\n",
      "268 0.03354566544294357\n",
      "269 0.03189947456121445\n",
      "270 0.03032645955681801\n",
      "271 0.02883707731962204\n",
      "272 0.027437174692749977\n",
      "273 0.02608591690659523\n",
      "274 0.024812616407871246\n",
      "275 0.023592159152030945\n",
      "276 0.022435234859585762\n",
      "277 0.02133692428469658\n",
      "278 0.02030530571937561\n",
      "279 0.019319066777825356\n",
      "280 0.01838076114654541\n",
      "281 0.017483994364738464\n",
      "282 0.016639331355690956\n",
      "283 0.015837041661143303\n",
      "284 0.015077795833349228\n",
      "285 0.01434424240142107\n",
      "286 0.013658055104315281\n",
      "287 0.01299434807151556\n",
      "288 0.012375845573842525\n",
      "289 0.011785033158957958\n",
      "290 0.011216224171221256\n",
      "291 0.010684090666472912\n",
      "292 0.010172003880143166\n",
      "293 0.009695181623101234\n",
      "294 0.009232684969902039\n",
      "295 0.008790167979896069\n",
      "296 0.00837142113596201\n",
      "297 0.007981179282069206\n",
      "298 0.007615217007696629\n",
      "299 0.007259106729179621\n",
      "300 0.006915528327226639\n",
      "301 0.006594059057533741\n",
      "302 0.006291575264185667\n",
      "303 0.005998946726322174\n",
      "304 0.005724073387682438\n",
      "305 0.005457782186567783\n",
      "306 0.005212380550801754\n",
      "307 0.004968998488038778\n",
      "308 0.00474359979853034\n",
      "309 0.00452821422368288\n",
      "310 0.004331297241151333\n",
      "311 0.00413712440058589\n",
      "312 0.003949094098061323\n",
      "313 0.003770928829908371\n",
      "314 0.0036039017140865326\n",
      "315 0.0034457335714250803\n",
      "316 0.003294713096693158\n",
      "317 0.0031512666027992964\n",
      "318 0.0030127379577606916\n",
      "319 0.0028839760925620794\n",
      "320 0.0027618478052318096\n",
      "321 0.002642229665070772\n",
      "322 0.0025318751577287912\n",
      "323 0.0024258450139313936\n",
      "324 0.002323585096746683\n",
      "325 0.002225153148174286\n",
      "326 0.0021317757200449705\n",
      "327 0.002043245593085885\n",
      "328 0.0019563701935112476\n",
      "329 0.0018773186020553112\n",
      "330 0.001800505560822785\n",
      "331 0.0017277870792895555\n",
      "332 0.0016612273175269365\n",
      "333 0.0015921841841191053\n",
      "334 0.0015277693746611476\n",
      "335 0.0014672457473352551\n",
      "336 0.0014093167847022414\n",
      "337 0.0013544104294851422\n",
      "338 0.0013020243495702744\n",
      "339 0.0012535960413515568\n",
      "340 0.001204031053930521\n",
      "341 0.001160681014880538\n",
      "342 0.0011145416647195816\n",
      "343 0.001073731342330575\n",
      "344 0.0010340726003050804\n",
      "345 0.0009950392413884401\n",
      "346 0.0009581050253473222\n",
      "347 0.0009241700172424316\n",
      "348 0.0008914688369259238\n",
      "349 0.0008585582254454494\n",
      "350 0.0008281369227916002\n",
      "351 0.0007984659750945866\n",
      "352 0.000770921353250742\n",
      "353 0.0007444593356922269\n",
      "354 0.0007181253167800605\n",
      "355 0.0006925711641088128\n",
      "356 0.0006692705792374909\n",
      "357 0.0006465758197009563\n",
      "358 0.0006245756521821022\n",
      "359 0.0006039446452632546\n",
      "360 0.0005834587500430644\n",
      "361 0.0005648431251756847\n",
      "362 0.0005464403075166047\n",
      "363 0.0005291285342536867\n",
      "364 0.0005130189820192754\n",
      "365 0.0004959217039868236\n",
      "366 0.00047981186071410775\n",
      "367 0.00046486244536936283\n",
      "368 0.0004503409727476537\n",
      "369 0.00043604709208011627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "370 0.0004226124146953225\n",
      "371 0.0004101716913282871\n",
      "372 0.00039761903462931514\n",
      "373 0.0003858095151372254\n",
      "374 0.0003740307001862675\n",
      "375 0.00036375655326992273\n",
      "376 0.00035318531445227563\n",
      "377 0.0003421108121983707\n",
      "378 0.00033289127168245614\n",
      "379 0.0003230060974601656\n",
      "380 0.0003136640880256891\n",
      "381 0.0003049702208954841\n",
      "382 0.0002967094478663057\n",
      "383 0.0002878496306948364\n",
      "384 0.00027999363373965025\n",
      "385 0.000272721576038748\n",
      "386 0.0002655550488270819\n",
      "387 0.00025847918004728854\n",
      "388 0.0002515523519832641\n",
      "389 0.00024519854923710227\n",
      "390 0.00023878642241470516\n",
      "391 0.0002322124200873077\n",
      "392 0.00022647152945864946\n",
      "393 0.0002207747456850484\n",
      "394 0.00021540505986195058\n",
      "395 0.0002094554074574262\n",
      "396 0.00020385533571243286\n",
      "397 0.00019937986508011818\n",
      "398 0.00019474170403555036\n",
      "399 0.00018921207811217755\n",
      "400 0.00018494410323910415\n",
      "401 0.00018098877626471221\n",
      "402 0.00017600991122890264\n",
      "403 0.00017165187455248088\n",
      "404 0.00016794964903965592\n",
      "405 0.0001644461153773591\n",
      "406 0.00016084901290014386\n",
      "407 0.00015647961117792875\n",
      "408 0.00015346196596510708\n",
      "409 0.00014981982531026006\n",
      "410 0.00014611198275815696\n",
      "411 0.00014316089800558984\n",
      "412 0.00013940614007879049\n",
      "413 0.00013629093882627785\n",
      "414 0.00013302030856721103\n",
      "415 0.00013062544167041779\n",
      "416 0.00012804794823750854\n",
      "417 0.00012535491259768605\n",
      "418 0.00012257046182639897\n",
      "419 0.00011971822823397815\n",
      "420 0.00011763373186113313\n",
      "421 0.00011539857950992882\n",
      "422 0.00011257681035203859\n",
      "423 0.00011004536645486951\n",
      "424 0.00010820569877978414\n",
      "425 0.00010591156024020165\n",
      "426 0.00010394363198429346\n",
      "427 0.00010163190017919987\n",
      "428 9.98182367766276e-05\n",
      "429 9.819002298172563e-05\n",
      "430 9.590267291059718e-05\n",
      "431 9.440627036383376e-05\n",
      "432 9.243370004696772e-05\n",
      "433 9.052900713868439e-05\n",
      "434 8.922733832150698e-05\n",
      "435 8.750676352065057e-05\n",
      "436 8.597626583650708e-05\n",
      "437 8.466521830996498e-05\n",
      "438 8.261258335551247e-05\n",
      "439 8.151949441526085e-05\n",
      "440 7.990484300535172e-05\n",
      "441 7.858443859731779e-05\n",
      "442 7.722988084424287e-05\n",
      "443 7.574826304335147e-05\n",
      "444 7.475474558304995e-05\n",
      "445 7.330848166020587e-05\n",
      "446 7.240488776005805e-05\n",
      "447 7.13166591594927e-05\n",
      "448 7.027827814454213e-05\n",
      "449 6.891626981087029e-05\n",
      "450 6.756657967343926e-05\n",
      "451 6.634717283304781e-05\n",
      "452 6.531613325932994e-05\n",
      "453 6.434934039134532e-05\n",
      "454 6.345954170683399e-05\n",
      "455 6.234308966668323e-05\n",
      "456 6.124278297647834e-05\n",
      "457 6.036779086571187e-05\n",
      "458 5.9326132031856105e-05\n",
      "459 5.8201319916406646e-05\n",
      "460 5.74814694118686e-05\n",
      "461 5.637039430439472e-05\n",
      "462 5.5360902479151264e-05\n",
      "463 5.4523599828826264e-05\n",
      "464 5.393940591602586e-05\n",
      "465 5.309723928803578e-05\n",
      "466 5.219583908910863e-05\n",
      "467 5.1429844461381435e-05\n",
      "468 5.0736347475321963e-05\n",
      "469 5.01758222526405e-05\n",
      "470 4.9601163482293487e-05\n",
      "471 4.879547486780211e-05\n",
      "472 4.8020450776675716e-05\n",
      "473 4.7495450417045504e-05\n",
      "474 4.6792280045337975e-05\n",
      "475 4.6179739001672715e-05\n",
      "476 4.555282066576183e-05\n",
      "477 4.501493094721809e-05\n",
      "478 4.446566163096577e-05\n",
      "479 4.372353942017071e-05\n",
      "480 4.314930993132293e-05\n",
      "481 4.256338070263155e-05\n",
      "482 4.201644696877338e-05\n",
      "483 4.151605025981553e-05\n",
      "484 4.0900224121287465e-05\n",
      "485 4.043131411890499e-05\n",
      "486 4.004210859420709e-05\n",
      "487 3.927767102140933e-05\n",
      "488 3.882471355609596e-05\n",
      "489 3.8451122236438096e-05\n",
      "490 3.7842070014448836e-05\n",
      "491 3.7492020055651665e-05\n",
      "492 3.700257002492435e-05\n",
      "493 3.6624358472181484e-05\n",
      "494 3.6096251278650016e-05\n",
      "495 3.555379225872457e-05\n",
      "496 3.5109565942548215e-05\n",
      "497 3.473065953585319e-05\n",
      "498 3.431196455494501e-05\n",
      "499 3.393990846234374e-05\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:6\") # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random input and output data\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y\n",
    "    h = x.mm(w1)\n",
    "    h_relu = h.clamp(min=0)\n",
    "    y_pred = h_relu.mm(w2)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    print(t, loss)\n",
    "\n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5., 3.], device='cuda:6')\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:6\")\n",
    "dtype = torch.float\n",
    "test = torch.tensor([5, 3], device=device, dtype=dtype)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "embedding_dimension = 100\n",
    "dtype = torch.float\n",
    "# device = torch.device(\"cpu\")\n",
    "device = torch.device(\"cuda:6\") # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, vocabulary_size, embedding_dimension, vocabulary_size\n",
    "\n",
    "# Create input and output data from idx_pairs\n",
    "x = []\n",
    "y = []\n",
    "for pair in idx_pairs:\n",
    "    cur_x = []\n",
    "    cur_y = []\n",
    "    for i in range(0, vocabulary_size):\n",
    "        if(i == pair[0]):\n",
    "            cur_x.append(1)\n",
    "        else:\n",
    "            cur_x.append(0)\n",
    "        if(i == pair[1]):\n",
    "            cur_y.append(1)\n",
    "        else:\n",
    "            cur_y.append(0)\n",
    "    x.append(cur_x)\n",
    "    y.append(cur_y)\n",
    "\n",
    "print(len(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 15 15\n",
      "torch.Size([15, 100])\n",
      "0 42256.65234375\n"
     ]
    }
   ],
   "source": [
    "\n",
    "                \n",
    "\n",
    "# # Create random input and output data\n",
    "# x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "# y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype)\n",
    "print(H, D_in, D_out)\n",
    "learning_rate = 1e-3\n",
    "loss_val = 0\n",
    "for t in range(1):\n",
    "    # Forward pass: compute predicted y\n",
    "    for i in range(len(x)):\n",
    "        x_vector = torch.tensor(x[i], device=device, dtype=dtype)        \n",
    "        y_vector = torch.tensor(y[i], device=device, dtype=dtype)        \n",
    "        z1 = torch.matmul(w1, x)\n",
    "        z2 = torch.matmul(w2, z1)\n",
    "        log_softmax = F.log_softmax(z2, dim=0)\n",
    "        loss = F.nll_loss(log_softmax.view(1,-1), y_vector)\n",
    "        loss_val += loss.item()\n",
    "        loss.backward()\n",
    "#         print(w1.size())\n",
    "#         h_relu = h.clamp(min=0)\n",
    "#         y_pred = h_relu.mm(w2)\n",
    "\n",
    "        # Compute and print loss\n",
    "#         loss = (y_pred - y).pow(2).sum().item()\n",
    "        print(t, loss_val)\n",
    "        W1.data -= learning_rate * W1.grad.data\n",
    "        W2.data -= learning_rate * W2.grad.data\n",
    "\n",
    "        W1.grad.data.zero_()\n",
    "        W2.grad.data.zero_()\n",
    "\n",
    "        # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "#         grad_y_pred = 2.0 * (y_pred - y)\n",
    "#         grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "#         grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "#         grad_h = grad_h_relu.clone()\n",
    "#         grad_h[h < 0] = 0\n",
    "#         grad_w1 = x.t().mm(grad_h)\n",
    "\n",
    "#         # Update weights using gradient descent\n",
    "#         w1 -= learning_rate * grad_w1\n",
    "#         w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.1420e-02,  6.1293e-01, -1.3923e-01, -1.8433e-02, -6.6051e-01,\n",
      "         1.1946e-01,  1.3269e+00, -5.1187e-02, -5.2783e-01, -1.3794e-01,\n",
      "        -9.5055e-02, -6.1692e-01, -1.9752e-01,  6.8335e-01,  6.0173e-01,\n",
      "        -1.2278e-01, -3.3696e-01, -2.2094e-01, -4.1712e-01, -7.6910e-01,\n",
      "        -5.6342e-01,  1.2546e-01,  7.3180e-01,  3.4895e-01,  1.6103e-01,\n",
      "        -1.7344e-01, -8.9016e-04,  7.0031e-02,  9.9244e-01, -7.3614e-01,\n",
      "        -9.4316e-01,  2.6637e-01, -6.9867e-01, -5.7772e-01, -4.3356e-01,\n",
      "        -1.4589e+00, -7.3039e-01, -1.5080e+00, -6.0735e-02,  1.0355e-01,\n",
      "        -2.2269e-01, -1.1411e+00, -1.6334e-01, -1.4689e+00, -3.1832e-01,\n",
      "        -1.7231e-01, -3.4880e-01, -1.3733e-01, -2.1905e-01,  7.7471e-01,\n",
      "         4.9175e-01,  5.5166e-01, -3.6932e-01,  4.7385e-01, -6.9067e-02,\n",
      "        -1.1892e-01, -2.2312e+00,  1.5254e-01, -5.2823e-02, -1.3127e+00,\n",
      "        -9.7139e-02, -1.1388e-01, -7.0674e-01, -7.9843e-02, -1.0846e+00,\n",
      "         7.5075e-01,  7.3198e-01, -7.9156e-01, -6.2829e-02, -7.8892e-01,\n",
      "        -1.2911e+00, -1.1714e-01, -1.0481e+00, -5.7731e-02, -1.9193e-02,\n",
      "         2.6836e-01,  4.5583e-01, -6.0798e-01,  1.1409e+00,  1.1897e+00,\n",
      "        -1.1454e-01, -3.6881e-01, -2.7615e-01, -5.3014e-01, -2.6818e-01,\n",
      "        -1.4057e-01, -7.5650e-01,  1.4195e-01, -7.1592e-01,  4.2864e-01,\n",
      "        -1.2410e-01,  5.1867e-01, -1.0417e-01, -3.5413e-01,  2.6051e-01,\n",
      "        -6.2556e-01,  1.2734e-01, -6.7777e-01,  7.0148e-02, -1.4558e-01],\n",
      "       device='cuda:6')\n"
     ]
    }
   ],
   "source": [
    "print(w1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
