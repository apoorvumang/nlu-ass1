{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting ids\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "def getTestTrain(fileds):\n",
    "    testids = []\n",
    "    trainids = []\n",
    "    for id in fileids:\n",
    "        tokens = id.split('/')\n",
    "        if tokens[0] == 'training':\n",
    "            trainids.append(id)\n",
    "        else:\n",
    "            if tokens[0] == 'test':\n",
    "                testids.append(id)\n",
    "    return testids, trainids\n",
    "\n",
    "def RepresentsInt(s):\n",
    "    try: \n",
    "        int(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "# remove numbers. they can be like 100 1090,200 2.123 etc\n",
    "# strategy is to remove punctuation and then check if its an integer\n",
    "def isNumber(word):\n",
    "    word_no_num = re.sub(r'[^\\w\\s]','',word)\n",
    "    if RepresentsInt(word_no_num):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "#tokenizes raw strings\n",
    "def getTokenized(ids):\n",
    "    exclude = set(string.punctuation)\n",
    "    words_list = []\n",
    "    for id in ids:\n",
    "        raw = reuters.raw(id)\n",
    "        words = nltk.word_tokenize(raw)\n",
    "        words_nopunc_nonum = []\n",
    "        for word in words:\n",
    "            if word in exclude: # if punctuation\n",
    "                continue\n",
    "            else:\n",
    "                word = word.lower()\n",
    "                if(isNumber(word)): # if number\n",
    "                    continue\n",
    "                words_nopunc_nonum.append(word)\n",
    "        words_list.append(words_nopunc_nonum)\n",
    "    return words_list\n",
    "\n",
    "\n",
    "def getVocabulary(tokenized_corpus):\n",
    "    vocabulary = {}\n",
    "    for sentence in tokenized_corpus:\n",
    "        for token in sentence:\n",
    "            if token not in vocabulary:\n",
    "                vocabulary[token] = 1\n",
    "            else:\n",
    "                vocabulary[token] += 1\n",
    "    new_vocab = {}\n",
    "    for key, value in vocabulary.items():\n",
    "        if(value >= 2):\n",
    "            new_vocab[key] = value\n",
    "    word2id = {w: idx for (idx, w) in enumerate(new_vocab)}\n",
    "    id2word = {idx: w for (idx, w) in enumerate(new_vocab)}\n",
    "    return new_vocab, word2id, id2word\n",
    "\n",
    "def generatePairs(sentences_tokenized, word2id, id2word):\n",
    "    window_size = 3\n",
    "    idx_pairs = []\n",
    "    # for each sentence\n",
    "    for sentence in sentences_tokenized:\n",
    "        indices = [word2id[word] for word in sentence]\n",
    "        # for each word, threated as center word\n",
    "        for center_word_pos in range(len(indices)):\n",
    "            # for each window position\n",
    "            for w in range(-window_size, window_size + 1):\n",
    "                context_word_pos = center_word_pos + w\n",
    "                # make soure not jump out sentence\n",
    "                if context_word_pos < 0 or context_word_pos >= len(indices) or center_word_pos == context_word_pos:\n",
    "                    continue\n",
    "                context_word_idx = indices[context_word_pos]\n",
    "                idx_pairs.append((indices[center_word_pos], context_word_idx))\n",
    "    return idx_pairs\n",
    "    \n",
    "def writeToFile(vocabulary, word2id, id2word, pairs):\n",
    "    f1 = open('data/vocab_hashnum_reduced.txt', 'w')\n",
    "    f2 = open('data/word2id_hashnum_reduced.txt', 'w')\n",
    "    f3 = open('data/id2word_hashnum_reduced.txt', 'w')\n",
    "\n",
    "    for key, value in vocabulary.items():\n",
    "        f1.write(key.encode('ascii', 'ignore').decode('ascii') + '\\t' + str(value) + '\\n')\n",
    "    for key, value in word2id.items():\n",
    "        f2.write(key.encode('ascii', 'ignore').decode('ascii') + '\\t' + str(value) + '\\n')\n",
    "    for key, value in id2word.items():\n",
    "        f3.write(str(key) + '\\t' + value.encode('ascii', 'ignore').decode('ascii') + '\\n')\n",
    "\n",
    "    f4 = open(\"data/pairs_subsampled_hashnum_win3_reduced.txt\", \"w\")\n",
    "    for pair in pairs:\n",
    "        f4.write(str(pair[0]) + '\\t' + str(pair[1]) + '\\n')\n",
    "    f4.close()\n",
    "    f3.close()\n",
    "    f2.close()\n",
    "    f1.close()\n",
    "\n",
    "\n",
    "print('Getting ids')\n",
    "fileids = reuters.fileids()\n",
    "testids, trainids = getTestTrain(fileids)\n",
    "print('Tokenizing')\n",
    "# sentences_tokenized = getTokenized(trainids)\n",
    "sentences_tokenized = getTokenized(fileids)\n",
    "\n",
    "print('Generating vocab')\n",
    "vocabulary, word2id, id2word = getVocabulary(sentences_tokenized)\n",
    "\n",
    "sentences_tokenized_subsampled = []\n",
    "for sentence in sentences_tokenized:\n",
    "    new_sent = []\n",
    "    for word in sentence:\n",
    "        if word not in vocabulary:\n",
    "            continue\n",
    "        freq = vocabulary[word]/len(vocabulary)\n",
    "        temp = freq/0.001\n",
    "        prob_to_keep = (temp**0.5 + 1)/temp\n",
    "#         prob_to_keep = (0.00001/freq)**0.5\n",
    "        p = random.uniform(0, 1)\n",
    "        if(p > prob_to_keep):\n",
    "            continue\n",
    "        else:\n",
    "            new_sent.append(word)\n",
    "    sentences_tokenized_subsampled.append(new_sent)\n",
    "\n",
    "pairs_new = generatePairs(sentences_tokenized_subsampled, word2id, id2word)\n",
    "\n",
    "\n",
    "# print('Writing data to files')\n",
    "# writeToFile(vocabulary, word2id, id2word, pairs_new)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20048"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2058628"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pairs_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data to files\n"
     ]
    }
   ],
   "source": [
    "print('Writing data to files')\n",
    "writeToFile(vocabulary, word2id, id2word, pairs_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_new = generatePairs(sentences_tokenized_subsampled, word2id, id2word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1393224"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pairs_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
